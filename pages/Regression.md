---
layout: page
title: "Regression"
description: ""
---
{% include JB/setup %}


**Regression:** Let \\(D = \{ X_{i}, Y_{i}|X_{i} \in R^{n},y_{i} \in R \} \\) be some data (experience). Aim: model the relationship between the *dependent variable* \\(Y_{i}\\) and *independent variables* \\(X_{i}^{T} = [x_{i,1}, x_{i,2}, ..., x_{i,n}]\\).
\\(x_{i}\\) are also called:  regressors, exogenous variables, explanatory variables, covariates, input variables, predictor variables, or independent variables.
\\(Y_{i}\\) is also called: regressand, endogenous variable, response variable, measured variable, criterion variable, or dependent variable.
<span style="text-decoration: underline">Linear regression:</span> linear relationship is assumed between \\(Y\\) and \\(X\\). Then the <span style="text-decoration: underline">linear regression model, or hypothesis</span> which represents the linear relationship between \\(X\\) and \\(Y\\) can be formulated as follows:

\\[\Y = f(X) = \theta_{0} + \sum_{j=1}^{n}(x_{j}\theta_{j})\]

where coefficients \\(\theta_{j}\\)'s called unknown parameters, or model parameters. These parameters are often arranged in a vector and we denote it as \\(\Theta = [\theta_{0},\theta_{1},\theta_{2},...,\theta_{n}]^{T}\\).
Therefore, in this model, the regressand (\\(Y\\)) is a <span style="text-decoration: underline">linear combination</span> of the regressors (\\(x_{j}\\)).



**Figure 1.** Example of linear regression. Each plot shows exactly the same data but only from different point of views. Data was generated by \\(\Theta = [0,5,4]\\), with a uniformly distributed error. (Zero is the bias).

So, what we want is to calculate the model parameter \\(\theta\\) somehow. This process is called "learning".


###### Regression and Ordinary Least Squares method (OLS):

Typically, having some data (experience) \\(D = \{(X_{i},Y_{i})|X_{i} \in R^{n}, y_{i} \in R\}\\), it is possible to define a performance estimation on how well the parameters \\(\Theta\\) model the relationship between the dependent and the outcome variables. This performance measure is often called loss function. The most common measure (in this field) is the *least squares* defined formally as:
\\[ J_{1}(\Theta|D) = \sum_{i=1}^{m}(Y_{i} - f(X_{i}))^{2} = \sum_{i=1}^{m}(Y_{i} - \theta_{0} - \sum_{j=1}^{n} x_{i,j}\theta_{j})^{2} \\]

Many other performance measures could be defined easily for any problem specific tasks. However, the reason why this method has become so popular is perhaps that it makes the loss function \\(J\\) a convex function with respect to parameters \\(\Theta\\). (The reason is not that, this loss function is reasonable.) This provides several numerical advantages. Note that, in practice other loss function might be more reasonable, but can make the optimization more difficult. See Figure 2 below.





**Figure 2.** The shape of the error function \\(J\\) w.r.t \\(D\\). Each plot shows exactly the same error function from different point of view, obtained from data shown in Figure 1. All three figures show the same function, but from different angle. \\(\theta_{0} = 0\\) was used.

**Regularized regression (ridge regression):** It imposes a penalty on the parameter values \\(\Theta\\).

\\[J(\Theta|D) = J_{1}(\Theta|D) + \lambda\sum_{j=1}^{n}\theta_{j}^{2}\\] 

The second term is called the regularization term, or penalty term. Here \\(\lambda\\) is a trade-off parameter that balances between the importance of regularization term and the fitting error. The larger the amount of \\(\lambda\\) the greater the importance of the regularization, and *vice versa*. The parameters shrink toward zero and and tush each other when \\(\lambda\\)  increases. Notice that \\(\theta_{0}\\) (the bias) is not included in the regularization term. The problem to find the \\(\widetilda{\Theta}\\), which minimizes the error can be formulated as follows:

\\[\widetilda{\Theta} = argmin_{\Theta}J(\Theta|D) = argmin_{\Theta}\{\sum_{i=1}^{m}(\theta_{0} + \sum_{j=1}^{n}x_{i,j}\theta_{j} - Y_{i})^{2} + \lambda\sum_{j=1}^{n} \theta_{j}^{2} \} \\]

Now, let's take step back for a moment and take a look at the two formulas (\\(F(x)\\) and \\(J(\Theta|D)\\). In the first one, a data \\(x\\) is a variable and the parameters \\(\theta_{i}\\) are constants. In the cost functions the data \\(x_{i}\\) becomes a constant and the model parameters are the variables. In fact, the optimal model parameters are some sort of combination of the data, or at least the optimal solution mostly depends on the data.

This error function is convex w.r.t. \\(\Theta\\), see an example on Figure 3.




**Figure 3.** The shape of the regularized error function \\(J\\) w.r.t. \\(D\\)  The first subplot corresponds to the error term, the second to the regularization term, and the last is the sum of the error and the regularization terms. Note that, the error function is different from the previous examples shown in Figures 4-5.


##### Solving fitting problems

###### Normal equations:
Let's arrange our data \\(x\\)'s in a matrix from. First, add a constant value 1 to each data:
\\[\\]

Then we want the \\(\Theta\\), which minimizes the least squares error of the following normal equations:
\\[(X^{T}X + \lambda I)\Theta = X^{T}Y \\]
We can rearrange the variables and give a closed form for \\(\Theta\\):
\\[\Theta = (X^{T}X + \lambda I)^{-1}X^{T}Y \\]
This solution involves matrix inversion which requires \\(O(n^{3})\\) time.

###### Gradient descent algorithm.
This method iteratively approaches an optima of functions. In each cycle 
it determines the tangent of the function in the current position in order to 
determine the direction of the optima. 