---
layout: page
title: "Regression"
description: ""
---
{% include JB/setup %}


**Regression:** Let \\(D = \\{X_{i}, Y_{i} \mid X_{i} \in R^{n}, y_{i} \in R \\} \\) be some data (experience). Aim: model the relationship between the *dependent variable* \\(Y_{i}\\) and *independent variables* \\(X_{i}^{T} = [x_{i,1}, x_{i,2}, ..., x_{i,n}]\\).

\\(x_{i}\\) are also called:  regressors, exogenous variables, explanatory variables, covariates, input variables, predictor variables, or independent variables.

\\(Y_{i}\\) is also called: regressand, endogenous variable, response variable, measured variable, criterion variable, or dependent variable.
<u>Linear regression:</u> linear relationship is assumed between \\(Y\\) and \\(X\\). Then the <u>linear regression model, or hypothesis</u> which represents the linear relationship between \\(X\\) and \\(Y\\) can be formulated as follows:

\\[Y = f(X) = \theta_{0} + \sum_{j=1}^{n}(x_{j}\theta_{j})\]

where coefficients \\(\theta_{j}\\)'s called unknown parameters, or model parameters. These parameters are often arranged in a vector and we denote it as \\(\Theta = [\theta_{0},\theta_{1},\theta_{2},...,\theta_{n}]^{T}\\).
Therefore, in this model, the regressand (\\(Y\\)) is a <u>linear combination</u> of the regressors (\\(x_{j}\\)).



**Figure 1.** Example of linear regression. Each plot shows exactly the same data but only from different point of views. Data was generated by \\(\Theta = [0,5,4]\\), with a uniformly distributed error. (Zero is the bias).

So, what we want is to calculate the model parameter \\(\theta\\) somehow. This process is called "learning".


###### Regression and Ordinary Least Squares method (OLS):

Typically, having some data (experience) \\(D = \{(X_{i},Y_{i})|X_{i} \in R^{n}, y_{i} \in R\}\\), it is possible to define a performance estimation on how well the parameters \\(\Theta\\) model the relationship between the dependent and the outcome variables. This performance measure is often called loss function. The most common measure (in this field) is the *least squares* defined formally as:
\\[ J_{1}(\Theta|D) = \sum_{i=1}^{m}(Y_{i} - f(X_{i}))^{2} = \sum_{i=1}^{m}(Y_{i} - \theta_{0} - \sum_{j=1}^{n} x_{i,j}\theta_{j})^{2} \\]

Many other performance measures could be defined easily for any problem specific tasks. However, the reason why this method has become so popular is perhaps that it makes the loss function \\(J\\) a convex function with respect to parameters \\(\Theta\\). (The reason is not that, this loss function is reasonable.) This provides several numerical advantages. Note that, in practice other loss function might be more reasonable, but can make the optimization more difficult. See Figure 2 below.





**Figure 2.** The shape of the error function \\(J\\) w.r.t \\(D\\). Each plot shows exactly the same error function from different point of view, obtained from data shown in Figure 1. All three figures show the same function, but from different angle. \\(\theta_{0} = 0\\) was used.

**Regularized regression (ridge regression):** It imposes a penalty on the parameter values \\(\Theta\\).

\\[J(\Theta|D) = J_{1}(\Theta|D) + \lambda\sum_{j=1}^{n}\theta_{j}^{2}\\] 

The second term is called the regularization term, or penalty term. Here \\(\lambda\\) is a trade-off parameter that balances between the importance of regularization term and the fitting error. The larger the amount of \\(\lambda\\) the greater the importance of the regularization, and *vice versa*. The parameters shrink toward zero and and tush each other when \\(\lambda\\)  increases. Notice that \\(\theta_{0}\\) (the bias) is not included in the regularization term. The problem to find the \\(\widetilda{\Theta}\\), which minimizes the error can be formulated as follows:

\\[\widetilda{\Theta} = argmin_{\Theta}J(\Theta|D) = argmin_{\Theta}\{\sum_{i=1}^{m}(\theta_{0} + \sum_{j=1}^{n}x_{i,j}\theta_{j} - Y_{i})^{2} + \lambda\sum_{j=1}^{n} \theta_{j}^{2} \} \\]

Now, let's take step back for a moment and take a look at the two formulas (\\(F(x)\\) and \\(J(\Theta|D)\\). In the first one, a data \\(x\\) is a variable and the parameters \\(\theta_{i}\\) are constants. In the cost functions the data \\(x_{i}\\) becomes a constant and the model parameters are the variables. In fact, the optimal model parameters are some sort of combination of the data, or at least the optimal solution mostly depends on the data.

This error function is convex w.r.t. \\(\Theta\\), see an example on Figure 3.




**Figure 3.** The shape of the regularized error function \\(J\\) w.r.t. \\(D\\)  The first subplot corresponds to the error term, the second to the regularization term, and the last is the sum of the error and the regularization terms. Note that, the error function is different from the previous examples shown in Figures 4-5.


##### Solving fitting problems

###### Normal equations:
Let's arrange our data \\(x\\)'s in a matrix from. First, add a constant value 1 to each data:
\\[\\]

Then we want the \\(\Theta\\), which minimizes the least squares error of the following normal equations:
\\[(X^{T}X + \lambda I)\Theta = X^{T}Y \\]
We can rearrange the variables and give a closed form for \\(\Theta\\):
\\[\Theta = (X^{T}X + \lambda I)^{-1}X^{T}Y \\]
This solution involves matrix inversion which requires \\(O(n^{3})\\) time.

###### Gradient descent algorithm.
This method iteratively approaches an optima of functions. In each cycle 
it determines the tangent of the function in the current position in order to 
determine the direction of the optima. 


###### Algorithm 1.



This update rule is called LMS (Least mean squares) update rule, or Widrow-Hoff learning rule.

The parameter \\(\mu\\) is the learning rate, it controls the size of the step in each iteration.
Some technical issues:
1. When \\(\mu\\) is too small then the convergence becomes slow.
2. If \\(\mu\\) is too large, then divergence.
3. The magnitude of the update is proportional to the error term: \\((Y_{i} - \theta_{0} - \sum_{j}^{n}x_{\{x,j\}}\theta_{j})\\)
4. Feature normalization: the range of the feature values needs to be transformed to the same scale



**Figure 4.** An example of the convergence of the Gradient Descent algorithm when data is not normalized. It converges slowly along with the large scale features and diverges on the small scale features.

There are two common approaches, a) normalization and b) scaling:


###### Batch mode:

In the case above the gradient descent algorithm uses the whole dataset.  You can use less data in each iteration, say, you can chose randomly 1, 5, 10, â€¦ in each iteration, and calculate the \\(\Theta\\)'s. This can be often useful if you have lot's of data, or data access is expensive (over network, etc.)
We can repeatedly run through the training set, and each time we encounter a training example, we update the parameters according to the gradient of the error with respect to that single training example only. This algorithm is called stochastic gradient descent (also incremental gradient descent). It slows the convergence down, but uses less data and hence less memory in each iteration.



Figure 8. An example path of the stochastic gradient algorithm.

###### Other optimization methods:
	Other optimization methods can be used instead of Gradient Descent, such as:
	Newton's method,
	Conjugate gradient
	BFGS (Broyden-Fletcher-Goldfarb-Shanno)
	L-BFGS (Limited memory - BFGS)

Each method would take the same input and provide (more-or-less) the same optimum at the output.


**Discussion 1.**  Data comes from some clinical experiment, where 1000 (or 10,000) patients examined and for each patient you have 100,000 features. You need to explore a linear relationship in the data. Which method would you use: 1) Normal equations, or 2) Gradient Descent Optimization. Why?

**Discussion 2.**  Data comes from used car retail shop, where cars are described by 54 features and there are 10,000,000 of such data objects. You need to explore a linear relationship in the data. Which method would you use: 1) Normal equations, or 2) Gradient Descent Optimization. Why?


###### Comaprison of the Normal Equation method vs. Gradient Descent Optimization.

Both methods aims at finding the same solution, but these two are different in some certain computational aspects.
	1. Normal Equation method:
		a. (-) O(n^3) can be very slow for large number of features.
		b. (-) Involves matrix inversion and can be a problem when the matrix becomes singular.
		c. (+) Not iterative, and thus does not require leraning rate and termination condition to be specified in advance. 
	2. Gradient Descent Optimization:
		a. (-) It is an iterative method and requires to specify the termination condition advance.
		b. (-) Requires the specification of the learning rate parameter.
		c. (+) Easy to scale up for paralell computations, better for big data.
		d. (-) Requires feature scaling.
(+) Easy to extend to other types of loss functions, i.e. this is a more general method, it works with non-convex loss functions as well.


###### Problems with linear regression in general:

A) Noise, 
B) non-lienar relationship, 
C-D) outliers, (Image source: Wikipedia. Anscombe's quartet)